============================================================
pyspark.SparkContext
Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDD and broadcast variables on that cluster.
任何Spark程序都是SparkContext开始的，SparkContext的初始化需要一个SparkConf对象，SparkConf包含了Spark集群配置的各种参数(比如主节点的URL)。
在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。
#创建SparkContext
from pyspark import SparkContext
sc = SparkContext()
class pyspark.SparkContext (
    master = None,#我们需要连接的集群url
    appName = None, #工作项目名称
    sparkHome = None, #spark安装路径
    pyFiles = None,#一般为处理文件的路径
    environment = None, #worker节点的环境变量
    batchSize = 0, 
    serializer = PickleSerializer(), #rdd序列化器
    conf = None, 
    gateway = None, #要么使用已经存在的JVM要么初始化一个新的JVM
    jsc = None, #JavaSparkContext实例
    profiler_cls = <class ‘pyspark.profiler.BasicProfiler‘>)
#SparkContext创建数据文件
sc.parallelize([1,2,3,4,5]) #创建RDD
sc.testFile('xxx.txt') #从HDFS读取文件创建RDD
sc.sql(query) #读取数据库数据
============================================================
pyspark.SparkConf
Configuration for a Spark application. Used to set various Spark parameters as key-value pairs.
Most of the time, you would create a SparkConf object with SparkConf(), which will load values from spark.* Java system properties as well. In this case, any parameters you set directly on the SparkConf object take priority over system properties.
#创建SparkConf
from pyspark import SparkConf
sc_conf=SparkConf().setAppName("miniProject").setMaster("local[*]").set('spark.executor.memory', '2g')...
sc = SparkContext(conf=sc_conf)
sc=SparkContext.getOrCreate(conf)
============================================================
 
SparkSession是Spark 2.0引入的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点。SparkSession实质上是SQLContext和HiveContext的组合(未来可能还会加上StreamingContext)，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了SparkContext，所以计算实际上是由SparkContext完成的。

